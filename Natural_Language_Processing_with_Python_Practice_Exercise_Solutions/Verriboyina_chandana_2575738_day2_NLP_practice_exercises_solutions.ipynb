{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "879ec749",
   "metadata": {},
   "source": [
    "# You are part of a team developing a text classification system for a news aggregator platform. The platform aims to categorize news articles into different topics automatically. The dataset contains news articles along with their corresponding topics. Perform only the Feature extraction techniques.\n",
    "\n",
    "Dataset Link: https://www.kaggle.com/datasets/therohk/million-headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497d6a0",
   "metadata": {},
   "source": [
    "# Data Exploration: \n",
    "\n",
    "Begin by exploring the dataset. What are the different topics/categories present in the dataset? What is the distribution of articles across these topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bd4464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37aed717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv('abcnews-date-text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47179864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      " ['aba decides against community broadcasting licence'\n",
      " 'act fire witnesses must be aware of defamation'\n",
      " 'a g calls for infrastructure protection summit' ...\n",
      " 'wa delays adopting new close contact definition'\n",
      " 'western ringtail possums found badly dehydrated in heatwave'\n",
      " 'what makes you a close covid contact here are the new rules']\n"
     ]
    }
   ],
   "source": [
    "# Explore the different topics/categories present in the dataset\n",
    "topics = df['headline_text'].unique()\n",
    "print(\"Topics:\\n\", topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0786c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Distribution:\n",
      " national rural news                                            983\n",
      "abc sport                                                      718\n",
      "abc weather                                                    714\n",
      "abc business news and market analysis                          585\n",
      "abc entertainment                                              551\n",
      "                                                              ... \n",
      "rio drug gang used alligators to terrify slum                    1\n",
      "research to identify women at risk of premature                  1\n",
      "religious order defends sex abuse handling                       1\n",
      "reigning champion federer advances to us semis                   1\n",
      "what makes you a close covid contact here are the new rules      1\n",
      "Name: headline_text, Length: 1213004, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Distribution of articles across topics\n",
    "topic_distribution = df['headline_text'].value_counts()\n",
    "print(\"Topic Distribution:\\n\", topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a78f6131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba decid commun broadcast licenc',\n",
       " 'act fire wit must awar defam',\n",
       " 'g call infrastructur protect summit',\n",
       " 'air nz staff aust strike pay rise',\n",
       " 'air nz strike affect australian travel',\n",
       " 'ambiti olsson win tripl jump',\n",
       " 'antic delight record break barca',\n",
       " 'aussi qualifi stosur wast four memphi match',\n",
       " 'aust address un secur council iraq',\n",
       " 'australia lock war timet opp']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a small portion of the data for illustration\n",
    "text_data = df.iloc[:1000]\n",
    "\n",
    "# Initialize NLTK components\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(text_data)):\n",
    "    text = text_data['headline_text'].iloc[i].lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [ps.stem(word) for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    corpus.append(text)\n",
    "\n",
    "new_text_data = corpus\n",
    "new_text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce04e0",
   "metadata": {},
   "source": [
    "# Bag-of-Words (BoW):\n",
    "\n",
    "Implement a Bag-of-Words (BoW) model using Count Vectorizer or TF-IDF to transform the text data into numerical features. Discuss the advantages and limitations of Bow in this context. Apply both unigram and bigram techniques and compare their effects on classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad84ce35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (BoW - Unigram): 0.0\n",
      "Accuracy (BoW - Bigram): 0.0\n",
      "\n",
      "Unigram BoW:\n",
      "\n",
      "Vocabulary Size: 2205\n",
      "Shape of BoW Matrix: (1000, 2205)\n",
      "BoW Feature Names:\n",
      " ['10' '100th' '108' ... 'zealand' 'zimbabw' 'zone']\n",
      "\n",
      "Bigram BoW:\n",
      "\n",
      "Vocabulary Size: 6226\n",
      "Shape of BoW Matrix: (1000, 6226)\n",
      "BoW Feature Names:\n",
      " ['10' '10 day' '10 man' ... 'zimbabw world' 'zone' 'zone home']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a feature matrix using Count Vectorizer (unigrams)\n",
    "cv = CountVectorizer()\n",
    "X_cv = cv.fit_transform(corpus)\n",
    "\n",
    "# Create a feature matrix using Count Vectorizer (bigrams)\n",
    "cv_bigram = CountVectorizer(ngram_range=(1, 2))\n",
    "X_cv_bigram = cv_bigram.fit_transform(corpus)\n",
    "y = text_data['headline_text']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_cv, X_test_cv, y_train, y_test = train_test_split(X_cv, y, test_size=0.2, random_state=42)\n",
    "X_train_cv_bigram, X_test_cv_bigram, _, _ = train_test_split(X_cv_bigram, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier (Naive Bayes is used here as an example)\n",
    "clf_cv = MultinomialNB()\n",
    "clf_cv.fit(X_train_cv, y_train)\n",
    "\n",
    "clf_cv_bigram = MultinomialNB()\n",
    "clf_cv_bigram.fit(X_train_cv_bigram, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_cv = clf_cv.predict(X_test_cv)\n",
    "y_pred_cv_bigram = clf_cv_bigram.predict(X_test_cv_bigram)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "print(\"Accuracy (BoW - Unigram):\", accuracy_score(y_test, y_pred_cv))\n",
    "print(\"Accuracy (BoW - Bigram):\", accuracy_score(y_test, y_pred_cv_bigram))\n",
    "\n",
    "# Display information about unigram BoW\n",
    "print(\"\\nUnigram BoW:\")\n",
    "print(\"\\nVocabulary Size:\", len(cv.vocabulary_))\n",
    "print(\"Shape of BoW Matrix:\", X_cv.shape)\n",
    "print(\"BoW Feature Names:\\n\", cv.get_feature_names_out())\n",
    "\n",
    "# Display information about bigram BoW\n",
    "print(\"\\nBigram BoW:\")\n",
    "print(\"\\nVocabulary Size:\", len(cv_bigram.vocabulary_))\n",
    "print(\"Shape of BoW Matrix:\", X_cv_bigram.shape)\n",
    "print(\"BoW Feature Names:\\n\", cv_bigram.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918964f",
   "metadata": {},
   "source": [
    "Advantages of Bag-of-Words (BoW):\n",
    "    \n",
    "   - Simplicity: BoW is a simple and effective way to represent text data.\n",
    "   - Interpretability: The resulting feature matrix is easy to interpret as it directly represents the occurrence of words.\n",
    "    \n",
    "Limitations of Bag-of-Words (BoW):\n",
    "    \n",
    "   - Lack of Semantic Understanding: BoW doesn't capture the semantic meaning of words and their relationships.\n",
    "   - High Dimensionality: In datasets with a large vocabulary, the feature matrix can become very high-dimensional.\n",
    "   - No Contextual Information: BoW treats each word independently, ignoring the order and structure of the words in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83bfba7",
   "metadata": {},
   "source": [
    "# N-grams: \n",
    "\n",
    "Explore the use of N-grams (bi-grams, tri-grams) in feature engineering. How do different N-gram ranges impact the performance of the classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7854d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " aba decid commun broadcast licenc act fire wit must awar defam g call infrastructur protect summit air nz staff aust strike pay rise air nz strike affect australian travel ambiti olsson win tripl jump antic delight record break barca aussi qualifi stosur wast four memphi match aust address un secur council iraq australia lock war timet opp\n",
      "\n",
      "Generated Bi-grams:\n",
      "('aba', 'decid')\n",
      "('decid', 'commun')\n",
      "('commun', 'broadcast')\n",
      "('broadcast', 'licenc')\n",
      "('licenc', 'act')\n",
      "('act', 'fire')\n",
      "('fire', 'wit')\n",
      "('wit', 'must')\n",
      "('must', 'awar')\n",
      "('awar', 'defam')\n",
      "('defam', 'g')\n",
      "('g', 'call')\n",
      "('call', 'infrastructur')\n",
      "('infrastructur', 'protect')\n",
      "('protect', 'summit')\n",
      "('summit', 'air')\n",
      "('air', 'nz')\n",
      "('nz', 'staff')\n",
      "('staff', 'aust')\n",
      "('aust', 'strike')\n",
      "('strike', 'pay')\n",
      "('pay', 'rise')\n",
      "('rise', 'air')\n",
      "('air', 'nz')\n",
      "('nz', 'strike')\n",
      "('strike', 'affect')\n",
      "('affect', 'australian')\n",
      "('australian', 'travel')\n",
      "('travel', 'ambiti')\n",
      "('ambiti', 'olsson')\n",
      "('olsson', 'win')\n",
      "('win', 'tripl')\n",
      "('tripl', 'jump')\n",
      "('jump', 'antic')\n",
      "('antic', 'delight')\n",
      "('delight', 'record')\n",
      "('record', 'break')\n",
      "('break', 'barca')\n",
      "('barca', 'aussi')\n",
      "('aussi', 'qualifi')\n",
      "('qualifi', 'stosur')\n",
      "('stosur', 'wast')\n",
      "('wast', 'four')\n",
      "('four', 'memphi')\n",
      "('memphi', 'match')\n",
      "('match', 'aust')\n",
      "('aust', 'address')\n",
      "('address', 'un')\n",
      "('un', 'secur')\n",
      "('secur', 'council')\n",
      "('council', 'iraq')\n",
      "('iraq', 'australia')\n",
      "('australia', 'lock')\n",
      "('lock', 'war')\n",
      "('war', 'timet')\n",
      "('timet', 'opp')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "# Combine sentences into a single text\n",
    "text = ' '.join(new_text_data)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Bi-grams\n",
    "bi_grams = list(ngrams(tokens, 2))\n",
    "\n",
    "print('Original text:\\n', text)\n",
    "print('\\nGenerated Bi-grams:')\n",
    "for grams in bi_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ffedbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Tri-grams:\n",
      "('aba', 'decid', 'commun')\n",
      "('decid', 'commun', 'broadcast')\n",
      "('commun', 'broadcast', 'licenc')\n",
      "('broadcast', 'licenc', 'act')\n",
      "('licenc', 'act', 'fire')\n",
      "('act', 'fire', 'wit')\n",
      "('fire', 'wit', 'must')\n",
      "('wit', 'must', 'awar')\n",
      "('must', 'awar', 'defam')\n",
      "('awar', 'defam', 'g')\n",
      "('defam', 'g', 'call')\n",
      "('g', 'call', 'infrastructur')\n",
      "('call', 'infrastructur', 'protect')\n",
      "('infrastructur', 'protect', 'summit')\n",
      "('protect', 'summit', 'air')\n",
      "('summit', 'air', 'nz')\n",
      "('air', 'nz', 'staff')\n",
      "('nz', 'staff', 'aust')\n",
      "('staff', 'aust', 'strike')\n",
      "('aust', 'strike', 'pay')\n",
      "('strike', 'pay', 'rise')\n",
      "('pay', 'rise', 'air')\n",
      "('rise', 'air', 'nz')\n",
      "('air', 'nz', 'strike')\n",
      "('nz', 'strike', 'affect')\n",
      "('strike', 'affect', 'australian')\n",
      "('affect', 'australian', 'travel')\n",
      "('australian', 'travel', 'ambiti')\n",
      "('travel', 'ambiti', 'olsson')\n",
      "('ambiti', 'olsson', 'win')\n",
      "('olsson', 'win', 'tripl')\n",
      "('win', 'tripl', 'jump')\n",
      "('tripl', 'jump', 'antic')\n",
      "('jump', 'antic', 'delight')\n",
      "('antic', 'delight', 'record')\n",
      "('delight', 'record', 'break')\n",
      "('record', 'break', 'barca')\n",
      "('break', 'barca', 'aussi')\n",
      "('barca', 'aussi', 'qualifi')\n",
      "('aussi', 'qualifi', 'stosur')\n",
      "('qualifi', 'stosur', 'wast')\n",
      "('stosur', 'wast', 'four')\n",
      "('wast', 'four', 'memphi')\n",
      "('four', 'memphi', 'match')\n",
      "('memphi', 'match', 'aust')\n",
      "('match', 'aust', 'address')\n",
      "('aust', 'address', 'un')\n",
      "('address', 'un', 'secur')\n",
      "('un', 'secur', 'council')\n",
      "('secur', 'council', 'iraq')\n",
      "('council', 'iraq', 'australia')\n",
      "('iraq', 'australia', 'lock')\n",
      "('australia', 'lock', 'war')\n",
      "('lock', 'war', 'timet')\n",
      "('war', 'timet', 'opp')\n"
     ]
    }
   ],
   "source": [
    "# Tri-grams\n",
    "tri_grams = list(ngrams(tokens, 3))\n",
    "\n",
    "print('\\nGenerated Tri-grams:')\n",
    "for grams in tri_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135119cb",
   "metadata": {},
   "source": [
    "# TF-IDF: \n",
    "\n",
    "Apply TF-IDF (Term Frequency-Inverse Document Frequency) to the text data. Describe how TF-IDF works and its significance in capturing the importance of words across documents. Compare the results of TF-IDF with the BoW approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efbbe46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Names:\n",
      "\n",
      "Vocabulary Size: 2205\n",
      "Shape of TF-IDF Matrix: (1000, 2205)\n",
      "TF-IDF Feature Names:\n",
      " ['10' '100th' '108' ... 'zealand' 'zimbabw' 'zone']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Create TF-IDF feature matrix\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get feature names\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display information about TF-IDF\n",
    "print(\"TF-IDF Feature Names:\")\n",
    "print(\"\\nVocabulary Size:\", len(feature_names_tfidf))\n",
    "print(\"Shape of TF-IDF Matrix:\", X_tfidf.shape)\n",
    "print(\"TF-IDF Feature Names:\\n\", feature_names_tfidf)\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea09e5",
   "metadata": {},
   "source": [
    "# One-Hot Encoding: \n",
    "\n",
    "Investigate the application of One-Hot Encoding to encode categorical variables or labels. Can One-Hot Encoding be used directly for text classification? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c50d6347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Step 1 - Tokens\n",
    "tokens = [word for sent in new_text_data for word in sent.split()]  # Update: Split without stemming\n",
    "\n",
    "# Step 2 - Vocabulary\n",
    "vocab = list(set(tokens))  # Unique words in the text\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(categories=[vocab], sparse=False)\n",
    "\n",
    "# Perform the One-Hot Encoding\n",
    "one_hot_encoder = []\n",
    "for sent in new_text_data:\n",
    "    sent_encoded = []\n",
    "    for word in sent.lower().split():\n",
    "        if word in vocab:  # Check if the stemmed word is in the vocabulary\n",
    "            word_index = vocab.index(word)\n",
    "            word_vector = np.zeros(len(vocab))\n",
    "            word_vector[word_index] = 1\n",
    "            sent_encoded.append(word_vector)\n",
    "    one_hot_encoder.append(sent_encoded)\n",
    "\n",
    "for sent in one_hot_encoder:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9b563",
   "metadata": {},
   "source": [
    "# Deliverables:\n",
    "\n",
    "Present insights gathered from data exploration and discuss the impact of different feature engineering techniques (BoW, N-grams, TF-IDF, One-Hot Encoding). Provide recommendations for the best feature engineering strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876f2a6",
   "metadata": {},
   "source": [
    "Impact of Feature Engineering Techniques:\n",
    "\n",
    "Bag-of-Words (BoW):\n",
    "\n",
    "Advantages:\n",
    "Simplicity: BoW is straightforward to implement and interpret.\n",
    "Interpretability: The resulting feature matrix is easy to understand.\n",
    "Limitations:\n",
    "Lack of Semantic Understanding: BoW doesn't capture the semantic meaning of words.\n",
    "High Dimensionality: The feature matrix can become very high-dimensional in large vocabularies.\n",
    "No Contextual Information: BoW treats each word independently, ignoring word order.\n",
    "\n",
    "N-grams:\n",
    "\n",
    "Bi-grams and tri-grams capture relationships between adjacent and nearby words.\n",
    "Impact on Performance:\n",
    "Bi-grams and tri-grams may enhance the model's ability to capture context and semantic meaning.\n",
    "The choice of N-gram range should be based on the specific characteristics of the dataset.\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "\n",
    "How TF-IDF Works:\n",
    "TF-IDF captures the importance of words by considering both the frequency of a term in a document and its rarity across all documents.\n",
    "Significance:\n",
    "Highlights terms that are frequent in a specific document but rare in the entire corpus.\n",
    "Useful for identifying distinctive words that carry meaningful information.\n",
    "\n",
    "One-Hot Encoding:\n",
    "\n",
    "Issues Encountered:\n",
    "Stemming and preprocessing impacted the vocabulary for One-Hot Encoding.\n",
    "Ensuring that stemmed words are in the vocabulary is crucial.\n",
    "Applicability:\n",
    "One-Hot Encoding is generally not suitable for text classification due to high dimensionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
